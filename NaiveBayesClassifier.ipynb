{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56d1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbb4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My own simple feature extraction function\n",
    "def hashfeatures1(name,debug=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        name : a string representing the person's name to be hashed\n",
    "    \n",
    "    Output:\n",
    "        v: a feature vector representing the input string\n",
    "        v[0] = begins with vowel\n",
    "        v[1] = ends with vowel\n",
    "        v[2] = ends in 'a'\n",
    "        v[3] = ends in 'y'\n",
    "    \"\"\"\n",
    "    d = 4 # d: the number of dimensions to be in the feature vector\n",
    "    v = np.zeros(d)\n",
    "    \n",
    "    if name[0] == 'A' or name[0] == 'E' or name[0] == 'I' or name[0] == 'O' or name[0] == 'U' or name[0] == 'Y':\n",
    "        v[0] = 1\n",
    "    if name[-1] == 'a' or name[-1] == 'e' or name[-1] == 'i' or name[-1] == 'o' or name[-1] == 'u' or name[-1] == 'y':\n",
    "        v[1] = 1\n",
    "    if name[-1] == 'a':\n",
    "        v[2] = 1\n",
    "    if name[-1] == 'y':\n",
    "        v[3] = 1\n",
    "    if debug:\n",
    "        print(f\"Feature vector for {name}:\\n{v.astype(int)}\\n\")\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b91cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function opens a file and calls the hashfeatures function on it to convert its contents into feature vectors\n",
    "def name2features(filename, d=128, FIX=3, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        X : n feature vectors of dimension d, (nxd)\n",
    "    \"\"\"\n",
    "    \n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            names = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        names = filename.split('\\n')\n",
    "    n = len(names)\n",
    "    X = np.zeros((n, d))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures1(names[i])\n",
    "    #print(X)\n",
    "    return (X, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b3e9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTrainFeatures(dimension=4):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        dimension: desired dimension of the features\n",
    "    Output: \n",
    "        allfeatures: n feature vectors of dimensionality d (nxd)\n",
    "        alllabels: n labels (-1 = girl, +1 = boy) (n)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the data\n",
    "    girlsTrain = \"GirlsTrain.txt\"\n",
    "    boysTrain = \"BoysTrain.txt\"\n",
    "    girlfeatures, numgirls = name2features(girlsTrain,4)\n",
    "    boyfeatures, numboys = name2features(boysTrain,4)\n",
    "    allfeatures = np.concatenate([girlfeatures,boyfeatures])\n",
    "    \n",
    "    # Generate Labels\n",
    "    girllabels = np.ones(numgirls) * -1\n",
    "    boylabels = np.ones(numboys)\n",
    "    allLabels = np.concatenate([girllabels,boylabels])\n",
    "    \n",
    "    return (allfeatures,allLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92f61dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayesPY(X, Y):\n",
    "    \"\"\"\n",
    "    naivebayesPY(X, Y) returns [pos,neg]\n",
    "\n",
    "    Computation of P(Y)\n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (nx1)\n",
    "\n",
    "    Output:\n",
    "        pos: probability p(y=1)\n",
    "        neg: probability p(y=-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    Y = np.concatenate([Y, [-1,1]])\n",
    "    n = len(Y)\n",
    "    arrsum = np.sum(Y)\n",
    "    \n",
    "    # SOLUTION - based on system of linear equations, solved for posCount with substitution\n",
    "    # arrSum = 1(posCount) - 1(negCount)\n",
    "    # n = posCount + negCount\n",
    "    \n",
    "    posCount = (n + arrsum)/2\n",
    "    \n",
    "    pos = (posCount)/n\n",
    "    neg = (n-posCount)/n\n",
    "    \n",
    "    #print(\"Probability of +1 is\",pos)\n",
    "    #print(\"Probability of -1 is\",neg)\n",
    "    return [pos,neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04b2c19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4932084309133489, 0.506791569086651]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naivebayesPY(allfeatures,allLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad68289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayesPXY(X,Y):\n",
    "    \"\"\"\n",
    "    naivebayesPXY(X, Y) returns [posprob,negprob]\n",
    "    \n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (n)\n",
    "    \n",
    "    Output:\n",
    "        posprob: probability vector of p(x_alpha = 1|y=1)  (d)\n",
    "            probability that feature d is 1 (true, or observed), given that the observed name is 1 (boy)\n",
    "        negprob: probability vector of p(x_alpha = 1|y=-1) (d)\n",
    "            probability that feature d is 1 (true, or observed), given that the observed name is 0 (girl)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    n, d = X.shape\n",
    "    X = np.concatenate([X, np.ones((2,d)), np.zeros((2,d))])\n",
    "    Y = np.concatenate([Y, [-1,1,-1,1]])\n",
    "    \n",
    "    # identify observations which are boys (slicing/indexing)\n",
    "    # count total number of boys\n",
    "    # across all of the d features, count the number of boy observations that are HOT, divide by total num of boys\n",
    "    \n",
    "    # I think this could be accomplished with just a sum, once I use some condtion on the Y vector (==1) to index the \n",
    "    # appropriate part of X\n",
    "    \n",
    "    # identify observations which are girls(slicing/indexing)\n",
    "    #print(X)\n",
    "    #print(Y)\n",
    "    \n",
    "    # Boys, Y=1\n",
    "    boys = X[Y == 1]\n",
    "    #print(boys)\n",
    "    b,d = boys.shape\n",
    "    #print(\"There are\",b,\"boys with\",d,\"features each\")\n",
    "    #Want to sum down the columns\n",
    "    boys_hot = np.sum(boys,axis=0)\n",
    "    #print(boys_hot)\n",
    "    posprob = boys_hot / b\n",
    "    #print(posprob)\n",
    "    \n",
    "    # Girls, Y=-1\n",
    "    girls = X[Y == -1]\n",
    "    g,d = girls.shape\n",
    "    #print(\"There are\",g,\"girls with\",d,\"features each\")\n",
    "    girls_hot = np.sum(girls,axis=0)\n",
    "    negprob = girls_hot / g\n",
    "    \n",
    "    return [posprob,negprob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b927315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.19639469, 0.29601518, 0.01802657, 0.07305503]),\n",
       " array([0.26592798, 0.72299169, 0.37026777, 0.10710988])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naivebayesPXY(allfeatures,allLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72434cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(posprob, negprob, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    loglikelihood(posprob, negprob, X_test, Y_test) returns loglikelihood of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        posprob: conditional probabilities for the positive class (d)\n",
    "        negprob: conditional probabilities for the negative class (d)\n",
    "        X_test : features (nxd)\n",
    "        Y_test : labels (-1 or +1) (n)\n",
    "    \n",
    "    Output:\n",
    "        loglikelihood of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    #print(\"Posprob distribution is:\",posprob)\n",
    "    posprob_c = np.ones(posprob.shape) - posprob # complement of posprob\n",
    "    #print(\"Complement of Posprob distribution is:\",posprob_c)\n",
    "    \n",
    "    #print(\"Negprob distribution is:\",negprob)\n",
    "    negprob_c = np.ones(negprob.shape) - negprob # complement of negprob\n",
    "    #print(\"Complement of Negprob distribution is:\",negprob_c)\n",
    "    \n",
    "    #print(\"Matrix of feature vectors is\",testX)\n",
    "    X_test_inv = np.zeros(X_test.shape)\n",
    "    X_test_inv[X_test == 0] = 1 # create matrix of same shape as X_test, with 0's and 1's switched\n",
    "    #print(\"Inverted matrix of feature vectors is\",X_test_inv)\n",
    "    \n",
    "    probdist = np.zeros(X_test.shape)\n",
    "    probdist[Y_test==1] = posprob\n",
    "    probdist[Y_test==-1]= negprob\n",
    "    #print(probdist)\n",
    "\n",
    "    probdist_c = np.zeros(X_test.shape)\n",
    "    probdist_c[Y_test==1] = posprob_c\n",
    "    probdist_c[Y_test==-1]= negprob_c\n",
    "    #print(probdist_c)\n",
    "\n",
    "    loglike = np.multiply(X_test,probdist) + np.multiply(X_test_inv,probdist_c)\n",
    "    #print(\"Log likelihood is\\n\",loglike)\n",
    "    loglike = np.log(loglike)\n",
    "    #print(loglike)\n",
    "    loglike = np.sum(loglike,axis=1)\n",
    "    #print(loglike)\n",
    "    \n",
    "    return loglike\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64a7e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayes_pred(pos, neg, posprob, negprob, X_test):\n",
    "    \"\"\"\n",
    "    naivebayes_pred(pos, neg, posprob, negprob, X_test) returns the prediction of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        pos: class probability for the negative class\n",
    "        neg: class probability for the positive class\n",
    "        posprob: conditional probabilities for the positive class (d)\n",
    "        negprob: conditional probabilities for the negative class (d)\n",
    "        X_test : features (nxd)\n",
    "    \n",
    "    Output:\n",
    "        prediction of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    # call loglikelihood to test (1) +1 label and (2) -1 labels\n",
    "        # add log of each class prior prob to each resulting output\n",
    "    # combine the two resulting nx1 matrices\n",
    "    # call argmax along the specified axis\n",
    "    # use indexing to replace values in resulting array (replace 0 with 1, replace 1 with -1)\n",
    "    \n",
    "    n,d = X_test.shape\n",
    "    \n",
    "    plusonelabel = np.ones(n)\n",
    "    #print(plusonelabel)\n",
    "    pos_loglike = loglikelihood(posprob, negprob, X_test, plusonelabel)\n",
    "    #print(\"Log likelihood of positive class\\n\",pos_loglike)\n",
    "    pos_loglike = pos_loglike + np.log(pos)\n",
    "    #print(\"With class prior probability\\n\",pos_loglike)\n",
    "    \n",
    "    minusonelabel = -1 * np.ones(n)\n",
    "    #print(minusonelabel)\n",
    "    neg_loglike = loglikelihood(posprob, negprob, X_test, minusonelabel)\n",
    "    #print(\"Log likelihood of negative class\\n\",neg_loglike)\n",
    "    neg_loglike = neg_loglike + np.log(neg)\n",
    "    #print(\"With class prior probability\\n\",neg_loglike)\n",
    "    \n",
    "    combined = np.vstack((pos_loglike,neg_loglike))\n",
    "    #print(combined)\n",
    "    \n",
    "    best = np.argmax(combined,axis=0)\n",
    "    #print(best)\n",
    "    best[best == 1] = -1\n",
    "    best[best == 0] = 1\n",
    "    \n",
    "    #print(best)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d938a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1,  1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX = np.array([[1,1,1],[0,1,1],[1,1,0],[0,0,0]])\n",
    "testY = np.array([-1,-1,1,1])\n",
    "testpos, testneg = naivebayesPY(testX,testY)\n",
    "test_posprob, test_negprob = naivebayesPXY(testX,testY)\n",
    "\n",
    "#test_posprob = [0.5,  0.5,  0.25]\n",
    "#test_negprob = [0.5,  0.75, 0.75]\n",
    "naivebayes_pred(0.5,0.5,test_posprob,test_negprob,testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2467d051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training classifier ...\n",
      "Training error: 28.60%\n",
      "Please enter a baby name (press enter with empty box to stop prompt)>\n",
      "Emily\n",
      "Emily, I am sure you are a baby girl.\n",
      "\n",
      "Please enter a baby name (press enter with empty box to stop prompt)>\n",
      "Rachel\n",
      "Rachel, I am sure you are a baby boy.\n",
      "\n",
      "Please enter a baby name (press enter with empty box to stop prompt)>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DIMS = 128\n",
    "#DIMS = 175\n",
    "DIMS = 4\n",
    "print('Loading data ...')\n",
    "X,Y = genTrainFeatures(DIMS)\n",
    "print('Training classifier ...')\n",
    "pos, neg = naivebayesPY(X, Y)\n",
    "posprob, negprob = naivebayesPXY(X, Y)\n",
    "error = np.mean(naivebayes_pred(pos, neg, posprob, negprob, X) != Y)\n",
    "print('Training error: %.2f%%' % (100 * error))\n",
    "\n",
    "while True:\n",
    "    print('Please enter a baby name (press enter with empty box to stop prompt)>')\n",
    "    yourname = input()\n",
    "    if len(yourname) < 1:\n",
    "        break\n",
    "    xtest,numnames = name2features(yourname,d=DIMS,LoadFile=False)\n",
    "    pred = naivebayes_pred(pos, neg, posprob, negprob, xtest)\n",
    "    if pred > 0:\n",
    "        print(\"%s, I am sure you are a baby boy.\\n\" % yourname)\n",
    "    else:\n",
    "        print(\"%s, I am sure you are a baby girl.\\n\" % yourname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30865ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
