{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56d1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ee3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Cornell course CIS532 project content:\n",
    "# Purpose of function is to convert a name into a feature vector\n",
    "# This function extracts features in the form of prefixes and suffixes, and creates a hashed 1-hot encoding\n",
    "\n",
    "def hashfeatures(baby, d, FIX, debug=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        baby : a string representing the baby's name to be hashed\n",
    "        \n",
    "        d: the number of dimensions to be in the feature vector\n",
    "        \n",
    "        FIX: the number of chunks to extract and hash from each string\n",
    "        \n",
    "        debug: a bool for printing debug values (default False)\n",
    "    \n",
    "    Output:\n",
    "        v: a feature vector representing the input string\n",
    "    \"\"\"\n",
    "    \n",
    "    v = np.zeros(d)\n",
    "    for m in range(1, FIX+1):\n",
    "        prefix = baby[:m] + \">\"\n",
    "        P = hash(prefix) % d\n",
    "        v[P] = 1\n",
    "        \n",
    "        suffix = \"<\" + baby[-m:]\n",
    "        S = hash(suffix) % d\n",
    "        v[S] = 1\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Split {m}/{FIX}:\\t({prefix}, {suffix}),\\t1s at indices [{P}, {S}]\")\n",
    "    if debug:\n",
    "        print(f\"Feature vector for {baby}:\\n{v.astype(int)}\\n\")\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b91cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Cornell course CIS532 project content:\n",
    "# This function opens a file and calls the hashfeatures function on it to convert its contents into feature vectors\n",
    "def name2features(filename, d=128, FIX=3, LoadFile=True, debug=False):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        X : n feature vectors of dimension d, (nxd)\n",
    "    \"\"\"\n",
    "    \n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, d))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], d, FIX)\n",
    "    return (X, babynames) if debug else X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f61dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayesPY(X, Y):\n",
    "    \"\"\"\n",
    "    naivebayesPY(X, Y) returns [pos,neg]\n",
    "\n",
    "    Computation of P(Y)\n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (nx1)\n",
    "\n",
    "    Output:\n",
    "        pos: probability p(y=1)\n",
    "        neg: probability p(y=-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    Y = np.concatenate([Y, [-1,1]])\n",
    "    n = len(Y)\n",
    "    arrsum = np.sum(Y)\n",
    "    \n",
    "    # SOLUTION - based on system of linear equations, solved for posCount with substitution\n",
    "    # arrSum = 1(posCount) - 1(negCount)\n",
    "    # n = posCount + negCount\n",
    "    \n",
    "    posCount = (n + arrsum)/2\n",
    "    \n",
    "    pos = (posCount)/n\n",
    "    neg = (n-posCount)/n\n",
    "    \n",
    "    #print(\"Probability of +1 is\",pos)\n",
    "    #print(\"Probability of -1 is\",neg)\n",
    "    return [pos,neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b89becc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5555555555555556, 0.4444444444444444]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests for naivesbayesPY\n",
    "testY1 = np.array([-1,1,-1,1,-1,1,1]) # one more +1 than -1, so sum will be +1\n",
    "naivebayesPY(testY1, testY1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a3c2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4, 0.6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY2 = np.array([-1,-1,1]) # one more -1 than +1, so sum will be -1\n",
    "naivebayesPY(testY2, testY2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f29ef5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY3 = np.array([1,1,1]) # one more -1 than +1, so sum will be -1\n",
    "naivebayesPY(testY3, testY3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad68289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayesPXY(X,Y):\n",
    "    \"\"\"\n",
    "    naivebayesPXY(X, Y) returns [posprob,negprob]\n",
    "    \n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (n)\n",
    "    \n",
    "    Output:\n",
    "        posprob: probability vector of p(x_alpha = 1|y=1)  (d)\n",
    "            probability that feature d is 1 (true, or observed), given that the observed name is 1 (boy)\n",
    "        negprob: probability vector of p(x_alpha = 1|y=-1) (d)\n",
    "            probability that feature d is 1 (true, or observed), given that the observed name is 0 (girl)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    n, d = X.shape\n",
    "    X = np.concatenate([X, np.ones((2,d)), np.zeros((2,d))])\n",
    "    Y = np.concatenate([Y, [-1,1,-1,1]])\n",
    "    \n",
    "    # identify observations which are boys (slicing/indexing)\n",
    "    # count total number of boys\n",
    "    # across all of the d features, count the number of boy observations that are HOT, divide by total num of boys\n",
    "    \n",
    "    # I think this could be accomplished with just a sum, once I use some condtion on the Y vector (==1) to index the \n",
    "    # appropriate part of X\n",
    "    \n",
    "    # identify observations which are girls(slicing/indexing)\n",
    "    #print(X)\n",
    "    #print(Y)\n",
    "    \n",
    "    # Boys, Y=1\n",
    "    boys = X[Y == 1]\n",
    "    #print(boys)\n",
    "    b,d = boys.shape\n",
    "    #print(\"There are\",b,\"boys with\",d,\"features each\")\n",
    "    #Want to sum down the columns\n",
    "    boys_hot = np.sum(boys,axis=0)\n",
    "    #print(boys_hot)\n",
    "    posprob = boys_hot / b\n",
    "    #print(posprob)\n",
    "    \n",
    "    # Girls, Y=-1\n",
    "    girls = X[Y == -1]\n",
    "    g,d = girls.shape\n",
    "    #print(\"There are\",g,\"girls with\",d,\"features each\")\n",
    "    girls_hot = np.sum(girls,axis=0)\n",
    "    negprob = girls_hot / g\n",
    "    \n",
    "    return [posprob,negprob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a2cf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.25, 1.25, 1.25]), array([1.4, 1.4, 1.4])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for naivebayesPXY\n",
    "testX = np.array([[0,0,0],[1,1,1],[2,2,2],[3,3,3],[4,4,4]])\n",
    "testY = np.array([-1,1,-1,1,-1])\n",
    "naivebayesPXY(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72434cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(posprob, negprob, X_test):\n",
    "    \"\"\"\n",
    "    loglikelihood(posprob, negprob, X_test) returns loglikelihood of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        posprob: conditional probabilities for the positive class (d), probability of feature given class +1\n",
    "        negprob: conditional probabilities for the negative class (d), probability of feature given class -1\n",
    "        X_test : features (nxd)\n",
    "    \n",
    "    Output:\n",
    "        log-likelihood of each point in X_test belonging to either class (nx2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Probability of each point belonging in BOYS (+1)\n",
    "    llp = np.array([])\n",
    "    # TODO: implement loglikelihood\n",
    "    \n",
    "    # Probability of each point belonging in GIRLS (-1)\n",
    "    lln = np.array([])\n",
    "    \n",
    "    return (llp,lln)\n",
    "    \n",
    "    # Notes:\n",
    "    # Original function definition from class project had input Y_test, which are presumably labels for the test \n",
    "    # points. This didn't make sense to me, so I removed it.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a7e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayes_pred(pos, neg, posprob, negprob, X_test):\n",
    "    \"\"\"\n",
    "    naivebayes_pred(pos, neg, posprob, negprob, X_test) returns the prediction of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        pos: class probability for the negative class\n",
    "        neg: class probability for the positive class\n",
    "        posprob: conditional probabilities for the positive class (d)\n",
    "        negprob: conditional probabilities for the negative class (d)\n",
    "        X_test : features (nxd)\n",
    "    \n",
    "    Output:\n",
    "        prediction of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    # call loglikelihood to find two points of comparison.\n",
    "    llp,lln = loglikelihood(posprob, negprob, X_test)\n",
    "    \n",
    "    # add class prior probability to each of the P(x|Y) vectors\n",
    "    llp = llp + pos\n",
    "    lln = lln + neg\n",
    "    \n",
    "    # TO DO: compare llp and lln element-wise, returning class of the maximum between the two\n",
    "    \n",
    "    # Notes:\n",
    "    # Instead of loglikelihood returning a 2-tuple of nx1 vectors, it could return an nx2 vector, which would make it\n",
    "    # easier to make the final prediction (use np.argmax). It would mean adding the class prior probability would\n",
    "    # be slightly more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467d051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
